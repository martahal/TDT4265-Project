{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload \n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "from ssd.config.defaults import cfg\n",
    "from demo import run_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded configuration file configs/train_rdd2020_server.yaml\n",
      "Running with config:\n",
      "DATASETS:\n",
      "  TEST: ('rdd2020_val',)\n",
      "  TRAIN: ('rdd2020_train',)\n",
      "DATASET_DIR: datasets\n",
      "DATA_LOADER:\n",
      "  NUM_WORKERS: 4\n",
      "  PIN_MEMORY: True\n",
      "EVAL_STEP: 500\n",
      "INPUT:\n",
      "  IMAGE_SIZE: [300, 300]\n",
      "  PIXEL_MEAN: [123.675, 116.28, 103.53]\n",
      "  PIXEL_STD: [1, 1, 1]\n",
      "LOG_STEP: 10\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    INPUT_CHANNELS: 3\n",
      "    NAME: res_net\n",
      "    OUT_CHANNELS: (128, 256, 512, 512, 256, 128)\n",
      "    PRETRAINED: True\n",
      "  CENTER_VARIANCE: 0.1\n",
      "  NEG_POS_RATIO: 3\n",
      "  NUM_CLASSES: 5\n",
      "  PRIORS:\n",
      "    ASPECT_RATIOS: [[2], [2, 3], [2, 3], [2, 3], [2], [2]]\n",
      "    BOXES_PER_LOCATION: [4, 6, 6, 6, 4, 4]\n",
      "    CLIP: True\n",
      "    FEATURE_MAPS: [[38, 38], [19, 19], [10, 10], [5, 5], [3, 3], [1, 1]]\n",
      "    MAX_SIZES: [[60, 60], [111, 111], [162, 162], [213, 213], [264, 264], [315, 315]]\n",
      "    MIN_SIZES: [[30, 30], [60, 60], [111, 111], [162, 162], [213, 213], [264, 264]]\n",
      "    STRIDES: [[8, 8], [16, 16], [32, 32], [64, 64], [100, 100], [300, 300]]\n",
      "  SIZE_VARIANCE: 0.2\n",
      "  THRESHOLD: 0.5\n",
      "MODEL_SAVE_STEP: 500\n",
      "OUTPUT_DIR: outputs/rdd2020\n",
      "SOLVER:\n",
      "  BATCH_SIZE: 16\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.001\n",
      "  MAX_ITER: 120000\n",
      "  MOMENTUM: 0.9\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "TEST:\n",
      "  BATCH_SIZE: 10\n",
      "  CONFIDENCE_THRESHOLD: 0.01\n",
      "  MAX_PER_CLASS: -1\n",
      "  MAX_PER_IMAGE: 100\n",
      "  NMS_THRESHOLD: 0.45\n",
      "Detector initialized. Total Number of params:  21.62M\n",
      "Backbone number of parameters: 20.84M\n",
      "SSD Head number of parameters: 788.2K\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for SSDDetector:\n\tMissing key(s) in state_dict: \"backbone.model.conv1.weight\", \"backbone.model.bn1.weight\", \"backbone.model.bn1.bias\", \"backbone.model.bn1.running_mean\", \"backbone.model.bn1.running_var\", \"backbone.model.layer1.0.conv1.weight\", \"backbone.model.layer1.0.bn1.weight\", \"backbone.model.layer1.0.bn1.bias\", \"backbone.model.layer1.0.bn1.running_mean\", \"backbone.model.layer1.0.bn1.running_var\", \"backbone.model.layer1.0.conv2.weight\", \"backbone.model.layer1.0.bn2.weight\", \"backbone.model.layer1.0.bn2.bias\", \"backbone.model.layer1.0.bn2.running_mean\", \"backbone.model.layer1.0.bn2.running_var\", \"backbone.model.layer1.1.conv1.weight\", \"backbone.model.layer1.1.bn1.weight\", \"backbone.model.layer1.1.bn1.bias\", \"backbone.model.layer1.1.bn1.running_mean\", \"backbone.model.layer1.1.bn1.running_var\", \"backbone.model.layer1.1.conv2.weight\", \"backbone.model.layer1.1.bn2.weight\", \"backbone.model.layer1.1.bn2.bias\", \"backbone.model.layer1.1.bn2.running_mean\", \"backbone.model.layer1.1.bn2.running_var\", \"backbone.model.layer2.0.conv1.weight\", \"backbone.model.layer2.0.bn1.weight\", \"backbone.model.layer2.0.bn1.bias\", \"backbone.model.layer2.0.bn1.running_mean\", \"backbone.model.layer2.0.bn1.running_var\", \"backbone.model.layer2.0.conv2.weight\", \"backbone.model.layer2.0.bn2.weight\", \"backbone.model.layer2.0.bn2.bias\", \"backbone.model.layer2.0.bn2.running_mean\", \"backbone.model.layer2.0.bn2.running_var\", \"backbone.model.layer2.0.downsample.0.weight\", \"backbone.model.layer2.0.downsample.1.weight\", \"backbone.model.layer2.0.downsample.1.bias\", \"backbone.model.layer2.0.downsample.1.running_mean\", \"backbone.model.layer2.0.downsample.1.running_var\", \"backbone.model.layer2.1.conv1.weight\", \"backbone.model.layer2.1.bn1.weight\", \"backbone.model.layer2.1.bn1.bias\", \"backbone.model.layer2.1.bn1.running_mean\", \"backbone.model.layer2.1.bn1.running_var\", \"backbone.model.layer2.1.conv2.weight\", \"backbone.model.layer2.1.bn2.weight\", \"backbone.model.layer2.1.bn2.bias\", \"backbone.model.layer2.1.bn2.running_mean\", \"backbone.model.layer2.1.bn2.running_var\", \"backbone.model.layer3.0.conv1.weight\", \"backbone.model.layer3.0.bn1.weight\", \"backbone.model.layer3.0.bn1.bias\", \"backbone.model.layer3.0.bn1.running_mean\", \"backbone.model.layer3.0.bn1.running_var\", \"backbone.model.layer3.0.conv2.weight\", \"backbone.model.layer3.0.bn2.weight\", \"backbone.model.layer3.0.bn2.bias\", \"backbone.model.layer3.0.bn2.running_mean\", \"backbone.model.layer3.0.bn2.running_var\", \"backbone.model.layer3.0.downsample.0.weight\", \"backbone.model.layer3.0.downsample.1.weight\", \"backbone.model.layer3.0.downsample.1.bias\", \"backbone.model.layer3.0.downsample.1.running_mean\", \"backbone.model.layer3.0.downsample.1.running_var\", \"backbone.model.layer3.1.conv1.weight\", \"backbone.model.layer3.1.bn1.weight\", \"backbone.model.layer3.1.bn1.bias\", \"backbone.model.layer3.1.bn1.running_mean\", \"backbone.model.layer3.1.bn1.running_var\", \"backbone.model.layer3.1.conv2.weight\", \"backbone.model.layer3.1.bn2.weight\", \"backbone.model.layer3.1.bn2.bias\", \"backbone.model.layer3.1.bn2.running_mean\", \"backbone.model.layer3.1.bn2.running_var\", \"backbone.model.layer4.0.conv1.weight\", \"backbone.model.layer4.0.bn1.weight\", \"backbone.model.layer4.0.bn1.bias\", \"backbone.model.layer4.0.bn1.running_mean\", \"backbone.model.layer4.0.bn1.running_var\", \"backbone.model.layer4.0.conv2.weight\", \"backbone.model.layer4.0.bn2.weight\", \"backbone.model.layer4.0.bn2.bias\", \"backbone.model.layer4.0.bn2.running_mean\", \"backbone.model.layer4.0.bn2.running_var\", \"backbone.model.layer4.0.downsample.0.weight\", \"backbone.model.layer4.0.downsample.1.weight\", \"backbone.model.layer4.0.downsample.1.bias\", \"backbone.model.layer4.0.downsample.1.running_mean\", \"backbone.model.layer4.0.downsample.1.running_var\", \"backbone.model.layer4.1.conv1.weight\", \"backbone.model.layer4.1.bn1.weight\", \"backbone.model.layer4.1.bn1.bias\", \"backbone.model.layer4.1.bn1.running_mean\", \"backbone.model.layer4.1.bn1.running_var\", \"backbone.model.layer4.1.conv2.weight\", \"backbone.model.layer4.1.bn2.weight\", \"backbone.model.layer4.1.bn2.bias\", \"backbone.model.layer4.1.bn2.running_mean\", \"backbone.model.layer4.1.bn2.running_var\", \"backbone.model.fc.weight\", \"backbone.model.fc.bias\", \"backbone.feature_extractor.layer1.1.weight\", \"backbone.feature_extractor.layer1.1.bias\", \"backbone.feature_extractor.layer1.1.running_mean\", \"backbone.feature_extractor.layer1.1.running_var\", \"backbone.feature_extractor.layer1.4.0.conv1.weight\", \"backbone.feature_extractor.layer1.4.0.bn1.weight\", \"backbone.feature_extractor.layer1.4.0.bn1.bias\", \"backbone.feature_extractor.layer1.4.0.bn1.running_mean\", \"backbone.feature_extractor.layer1.4.0.bn1.running_var\", \"backbone.feature_extractor.layer1.4.0.conv2.weight\", \"backbone.feature_extractor.layer1.4.0.bn2.weight\", \"backbone.feature_extractor.layer1.4.0.bn2.bias\", \"backbone.feature_extractor.layer1.4.0.bn2.running_mean\", \"backbone.feature_extractor.layer1.4.0.bn2.running_var\", \"backbone.feature_extractor.layer1.4.1.conv1.weight\", \"backbone.feature_extractor.layer1.4.1.bn1.weight\", \"backbone.feature_extractor.layer1.4.1.bn1.bias\", \"backbone.feature_extractor.layer1.4.1.bn1.running_mean\", \"backbone.feature_extractor.layer1.4.1.bn1.running_var\", \"backbone.feature_extractor.layer1.4.1.conv2.weight\", \"backbone.feature_extractor.layer1.4.1.bn2.weight\", \"backbone.feature_extractor.layer1.4.1.bn2.bias\", \"backbone.feature_extractor.layer1.4.1.bn2.running_mean\", \"backbone.feature_extractor.layer1.4.1.bn2.running_var\", \"backbone.feature_extractor.layer1.5.0.conv1.weight\", \"backbone.feature_extractor.layer1.5.0.bn1.weight\", \"backbone.feature_extractor.layer1.5.0.bn1.bias\", \"backbone.feature_extractor.layer1.5.0.bn1.running_mean\", \"backbone.feature_extractor.layer1.5.0.bn1.running_var\", \"backbone.feature_extractor.layer1.5.0.conv2.weight\", \"backbone.feature_extractor.layer1.5.0.bn2.weight\", \"backbone.feature_extractor.layer1.5.0.bn2.bias\", \"backbone.feature_extractor.layer1.5.0.bn2.running_mean\", \"backbone.feature_extractor.layer1.5.0.bn2.running_var\", \"backbone.feature_extractor.layer1.5.0.downsample.0.weight\", \"backbone.feature_extractor.layer1.5.0.downsample.1.weight\", \"backbone.feature_extractor.layer1.5.0.downsample.1.bias\", \"backbone.feature_extractor.layer1.5.0.downsample.1.running_mean\", \"backbone.feature_extractor.layer1.5.0.downsample.1.running_var\", \"backbone.feature_extractor.layer1.5.1.conv1.weight\", \"backbone.feature_extractor.layer1.5.1.bn1.weight\", \"backbone.feature_extractor.layer1.5.1.bn1.bias\", \"backbone.feature_extractor.layer1.5.1.bn1.running_mean\", \"backbone.feature_extractor.layer1.5.1.bn1.running_var\", \"backbone.feature_extractor.layer1.5.1.conv2.weight\", \"backbone.feature_extractor.layer1.5.1.bn2.weight\", \"backbone.feature_extractor.layer1.5.1.bn2.bias\", \"backbone.feature_extractor.layer1.5.1.bn2.running_mean\", \"backbone.feature_extractor.layer1.5.1.bn2.running_var\", \"backbone.feature_extractor.layer2.0.conv1.weight\", \"backbone.feature_extractor.layer2.0.bn1.weight\", \"backbone.feature_extractor.layer2.0.bn1.bias\", \"backbone.feature_extractor.layer2.0.bn1.running_mean\", \"backbone.feature_extractor.layer2.0.bn1.running_var\", \"backbone.feature_extractor.layer2.0.conv2.weight\", \"backbone.feature_extractor.layer2.0.bn2.weight\", \"backbone.feature_extractor.layer2.0.bn2.bias\", \"backbone.feature_extractor.layer2.0.bn2.running_mean\", \"backbone.feature_extractor.layer2.0.bn2.running_var\", \"backbone.feature_extractor.layer2.0.downsample.0.weight\", \"backbone.feature_extractor.layer2.0.downsample.1.weight\", \"backbone.feature_extractor.layer2.0.downsample.1.bias\", \"backbone.feature_extractor.layer2.0.downsample.1.running_mean\", \"backbone.feature_extractor.layer2.0.downsample.1.running_var\", \"backbone.feature_extractor.layer2.1.conv1.weight\", \"backbone.feature_extractor.layer2.1.bn1.weight\", \"backbone.feature_extractor.layer2.1.bn1.bias\", \"backbone.feature_extractor.layer2.1.bn1.running_mean\", \"backbone.feature_extractor.layer2.1.bn1.running_var\", \"backbone.feature_extractor.layer2.1.conv2.weight\", \"backbone.feature_extractor.layer2.1.bn2.weight\", \"backbone.feature_extractor.layer2.1.bn2.bias\", \"backbone.feature_extractor.layer2.1.bn2.running_mean\", \"backbone.feature_extractor.layer2.1.bn2.running_var\", \"backbone.feature_extractor.layer3.0.conv1.weight\", \"backbone.feature_extractor.layer3.0.bn1.weight\", \"backbone.feature_extractor.layer3.0.bn1.bias\", \"backbone.feature_extractor.layer3.0.bn1.running_mean\", \"backbone.feature_extractor.layer3.0.bn1.running_var\", \"backbone.feature_extractor.layer3.0.conv2.weight\", \"backbone.feature_extractor.layer3.0.bn2.weight\", \"backbone.feature_extractor.layer3.0.bn2.bias\", \"backbone.feature_extractor.layer3.0.bn2.running_mean\", \"backbone.feature_extractor.layer3.0.bn2.running_var\", \"backbone.feature_extractor.layer3.0.downsample.0.weight\", \"backbone.feature_extractor.layer3.0.downsample.1.weight\", \"backbone.feature_extractor.layer3.0.downsample.1.bias\", \"backbone.feature_extractor.layer3.0.downsample.1.running_mean\", \"backbone.feature_extractor.layer3.0.downsample.1.running_var\", \"backbone.feature_extractor.layer3.1.conv1.weight\", \"backbone.feature_extractor.layer3.1.bn1.weight\", \"backbone.feature_extractor.layer3.1.bn1.bias\", \"backbone.feature_extractor.layer3.1.bn1.running_mean\", \"backbone.feature_extractor.layer3.1.bn1.running_var\", \"backbone.feature_extractor.layer3.1.conv2.weight\", \"backbone.feature_extractor.layer3.1.bn2.weight\", \"backbone.feature_extractor.layer3.1.bn2.bias\", \"backbone.feature_extractor.layer3.1.bn2.running_mean\", \"backbone.feature_extractor.layer3.1.bn2.running_var\". \n\tUnexpected key(s) in state_dict: \"backbone.feature_extractor.layer1.6.weight\", \"backbone.feature_extractor.layer1.6.bias\", \"backbone.feature_extractor.layer1.8.weight\", \"backbone.feature_extractor.layer1.8.bias\", \"backbone.feature_extractor.layer1.9.weight\", \"backbone.feature_extractor.layer1.9.bias\", \"backbone.feature_extractor.layer1.9.running_mean\", \"backbone.feature_extractor.layer1.9.running_var\", \"backbone.feature_extractor.layer1.9.num_batches_tracked\", \"backbone.feature_extractor.layer1.0.bias\", \"backbone.feature_extractor.layer1.3.weight\", \"backbone.feature_extractor.layer1.3.bias\", \"backbone.feature_extractor.layer2.3.weight\", \"backbone.feature_extractor.layer2.3.bias\", \"backbone.feature_extractor.layer2.4.weight\", \"backbone.feature_extractor.layer2.4.bias\", \"backbone.feature_extractor.layer2.4.running_mean\", \"backbone.feature_extractor.layer2.4.running_var\", \"backbone.feature_extractor.layer2.4.num_batches_tracked\", \"backbone.feature_extractor.layer2.1.weight\", \"backbone.feature_extractor.layer2.1.bias\", \"backbone.feature_extractor.layer3.3.weight\", \"backbone.feature_extractor.layer3.3.bias\", \"backbone.feature_extractor.layer3.4.weight\", \"backbone.feature_extractor.layer3.4.bias\", \"backbone.feature_extractor.layer3.4.running_mean\", \"backbone.feature_extractor.layer3.4.running_var\", \"backbone.feature_extractor.layer3.4.num_batches_tracked\", \"backbone.feature_extractor.layer3.1.weight\", \"backbone.feature_extractor.layer3.1.bias\". \n\tsize mismatch for backbone.feature_extractor.layer1.0.weight: copying a param with shape torch.Size([32, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 3, 7, 7]).\n\tsize mismatch for backbone.feature_extractor.layer4.1.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3]).\n\tsize mismatch for backbone.feature_extractor.layer4.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for backbone.feature_extractor.layer4.3.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3]).\n\tsize mismatch for backbone.feature_extractor.layer4.3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for backbone.feature_extractor.layer4.4.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for backbone.feature_extractor.layer4.4.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for backbone.feature_extractor.layer4.4.running_mean: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for backbone.feature_extractor.layer4.4.running_var: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for backbone.feature_extractor.layer5.1.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3]).\n\tsize mismatch for backbone.feature_extractor.layer5.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for backbone.feature_extractor.layer5.3.weight: copying a param with shape torch.Size([128, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 512, 3, 3]).\n\tsize mismatch for backbone.feature_extractor.layer5.3.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for backbone.feature_extractor.layer5.4.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for backbone.feature_extractor.layer5.4.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for backbone.feature_extractor.layer5.4.running_mean: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for backbone.feature_extractor.layer5.4.running_var: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for backbone.feature_extractor.layer6.1.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3]).\n\tsize mismatch for backbone.feature_extractor.layer6.1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for backbone.feature_extractor.layer6.3.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 256, 3, 3]).\n\tsize mismatch for box_head.predictor.cls_headers.0.weight: copying a param with shape torch.Size([20, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([20, 128, 3, 3]).\n\tsize mismatch for box_head.predictor.cls_headers.1.weight: copying a param with shape torch.Size([30, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([30, 256, 3, 3]).\n\tsize mismatch for box_head.predictor.cls_headers.2.weight: copying a param with shape torch.Size([30, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([30, 512, 3, 3]).\n\tsize mismatch for box_head.predictor.cls_headers.3.weight: copying a param with shape torch.Size([30, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([30, 512, 3, 3]).\n\tsize mismatch for box_head.predictor.cls_headers.4.weight: copying a param with shape torch.Size([20, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([20, 256, 3, 3]).\n\tsize mismatch for box_head.predictor.reg_headers.0.weight: copying a param with shape torch.Size([16, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 128, 3, 3]).\n\tsize mismatch for box_head.predictor.reg_headers.1.weight: copying a param with shape torch.Size([24, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([24, 256, 3, 3]).\n\tsize mismatch for box_head.predictor.reg_headers.2.weight: copying a param with shape torch.Size([24, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([24, 512, 3, 3]).\n\tsize mismatch for box_head.predictor.reg_headers.3.weight: copying a param with shape torch.Size([24, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([24, 512, 3, 3]).\n\tsize mismatch for box_head.predictor.reg_headers.4.weight: copying a param with shape torch.Size([16, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 256, 3, 3]).",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-3-d4f478d51af7>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     19\u001B[0m     \u001B[0mimages_dir\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mpathlib\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPath\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mimages_dir\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     20\u001B[0m     \u001B[0moutput_dir\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mpathlib\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPath\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mimages_dir\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"result\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 21\u001B[0;31m     dataset_type=dataset_type)\n\u001B[0m\u001B[1;32m     22\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/CV_DL_Project/lib/python3.6/site-packages/torch/autograd/grad_mode.py\u001B[0m in \u001B[0;36mdecorate_no_grad\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     47\u001B[0m         \u001B[0;32mdef\u001B[0m \u001B[0mdecorate_no_grad\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     48\u001B[0m             \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 49\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     50\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mdecorate_no_grad\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     51\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/CV_DL_Project/assignment4/SSD/demo.py\u001B[0m in \u001B[0;36mrun_demo\u001B[0;34m(cfg, ckpt, score_threshold, images_dir, output_dir, dataset_type)\u001B[0m\n\u001B[1;32m     30\u001B[0m     \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch_utils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto_cuda\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     31\u001B[0m     \u001B[0mcheckpointer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mCheckPointer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msave_dir\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcfg\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mOUTPUT_DIR\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 32\u001B[0;31m     \u001B[0mcheckpointer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mckpt\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0muse_latest\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mckpt\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     33\u001B[0m     \u001B[0mweight_file\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mckpt\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mckpt\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0mcheckpointer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_checkpoint_file\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     34\u001B[0m     \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'Loaded weights from {}'\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mweight_file\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/CV_DL_Project/assignment4/SSD/ssd/utils/checkpoint.py\u001B[0m in \u001B[0;36mload\u001B[0;34m(self, f, use_latest)\u001B[0m\n\u001B[1;32m     58\u001B[0m         \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     59\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 60\u001B[0;31m         \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload_state_dict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcheckpoint\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpop\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"model\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     61\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;34m\"optimizer\"\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mcheckpoint\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptimizer\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     62\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlogger\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minfo\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Loading optimizer from {}\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/CV_DL_Project/lib/python3.6/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36mload_state_dict\u001B[0;34m(self, state_dict, strict)\u001B[0m\n\u001B[1;32m    837\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0merror_msgs\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    838\u001B[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001B[0;32m--> 839\u001B[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001B[0m\u001B[1;32m    840\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0m_IncompatibleKeys\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmissing_keys\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0munexpected_keys\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    841\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Error(s) in loading state_dict for SSDDetector:\n\tMissing key(s) in state_dict: \"backbone.model.conv1.weight\", \"backbone.model.bn1.weight\", \"backbone.model.bn1.bias\", \"backbone.model.bn1.running_mean\", \"backbone.model.bn1.running_var\", \"backbone.model.layer1.0.conv1.weight\", \"backbone.model.layer1.0.bn1.weight\", \"backbone.model.layer1.0.bn1.bias\", \"backbone.model.layer1.0.bn1.running_mean\", \"backbone.model.layer1.0.bn1.running_var\", \"backbone.model.layer1.0.conv2.weight\", \"backbone.model.layer1.0.bn2.weight\", \"backbone.model.layer1.0.bn2.bias\", \"backbone.model.layer1.0.bn2.running_mean\", \"backbone.model.layer1.0.bn2.running_var\", \"backbone.model.layer1.1.conv1.weight\", \"backbone.model.layer1.1.bn1.weight\", \"backbone.model.layer1.1.bn1.bias\", \"backbone.model.layer1.1.bn1.running_mean\", \"backbone.model.layer1.1.bn1.running_var\", \"backbone.model.layer1.1.conv2.weight\", \"backbone.model.layer1.1.bn2.weight\", \"backbone.model.layer1.1.bn2.bias\", \"backbone.model.layer1.1.bn2.running_mean\", \"backbone.model.layer1.1.bn2.running_var\", \"backbone.model.layer2.0.conv1.weight\", \"backbone.model.layer2.0.bn1.weight\", \"backbone.model.layer2.0.bn1.bias\", \"backbone.model.layer2.0.bn1.running_mean\", \"backbone.model.layer2.0.bn1.running_var\", \"backbone.model.layer2.0.conv2.weight\", \"backbone.model.layer2.0.bn2.weight\", \"backbone.model.layer2.0.bn2.bias\", \"backbone.model.layer2.0.bn2.running_mean\", \"backbone.model.layer2.0.bn2.running_var\", \"backbone.model.layer2.0.downsample.0.weight\", \"backbone.model.layer2.0.downsample.1.weight\", \"backbone.model.layer2.0.downsample.1.bias\", \"backbone.model.layer2.0.downsample.1.running_mean\", \"backbone.model.layer2.0.downsample.1.running_var\", \"backbone.model.layer2.1.conv1.weight\", \"backbone.model.layer2.1.bn1.weight\", \"backbone.model.layer2.1.bn1.bias\", \"backbone.model.layer2.1.bn1.running_mean\", \"backbone.model.layer2.1.bn1.running_var\", \"backbone.model.layer2.1.conv2.weight\", \"backbone.model.layer2.1.bn2.weight\", \"backbone.model.layer2.1.bn2.bias\", \"backbone.model.layer2.1.bn2.running_mean\", \"backbone.model.layer2.1.bn2.running_var\", \"backbone.model.layer3.0.conv1.weight\", \"backbone.model.layer3.0.bn1.weight\", \"backbone.model.layer3.0.bn1.bias\", \"backbone.model.layer3.0.bn1.running_mean\", \"backbone.model.layer3.0.bn1.running_var\", \"backbone.model.layer3.0.conv2.weight\", \"backbone.model.layer3.0.bn2.weight\", \"backbone.model.layer3.0.bn2.bias\", \"backbone.model.layer3.0.bn2.running_mean\", \"backbone.model.layer3.0.bn2.running_var\", \"backbone.model.layer3.0.downsample.0.weight\", \"backbone.model.layer3.0.downsample.1.weight\", \"backbone.model.layer3.0.downsample.1.bias\", \"backbone.model.layer3.0.downsample.1.running_mean\", \"backbone.model.layer3.0.downsample.1.running_var\", \"backbone.model.layer3.1.conv1.weight\", \"backbone.model.layer3.1.bn1.weight\", \"backbone.model.layer3.1.bn1.bias\", \"backbone.model.layer3.1.bn1.running_mean\", \"backbone.model.layer3.1.bn1.running_var\", \"backbone.model.layer3.1.conv2.weight\", \"backbone.model.layer3.1.bn2.weight\", \"backbone.model.layer3.1.bn2.bias\", \"backbone.model.layer3.1.bn2.running_mean\", \"backbone.model.layer3.1.bn2.running_var\", \"backbone.model.layer4.0.conv1.weight\", \"backbone.model.layer4.0.bn1.weight\", \"backbone.model.layer4.0.bn1.bias\", \"backbone.model.layer4.0.bn1.running_mean\", \"backbone.model.layer4.0.bn1.running_var\", \"backbone.model.layer4.0.conv2.weight\", \"backbone.model.layer4.0.bn2.weight\", \"backbone.model.layer4.0.bn2.bias\", \"backbone.model.layer4.0.bn2.running_mean\", \"backbone.model.layer4.0.bn2.running_var\", \"backbone.model.layer4.0.downsample.0.weight\", \"backbone.model.layer4.0.downsample.1.weight\", \"backbone.model.layer4.0.downsample.1.bias\", \"backbone.model.layer4.0.downsample.1.running_mean\", \"backbone.model.layer4.0.downsample.1.running_var\", \"backbone.model.layer4.1.conv1.weight\", \"backbone.model.layer4.1.bn1.weight\", \"backbone.model.layer4.1.bn1.bias\", \"backbone.model.layer4.1.bn1.running_mean\", \"backbone.model.layer4.1.bn1.running_var\", \"backbone.model.layer4.1.conv2.weight\", \"backbone.model.layer4.1.bn2.weight\", \"backbone.model.layer4.1.bn2.bias\", \"backbone.model.layer4.1.bn2.running_mean\", \"backbone.model.layer4.1.bn2.running_var\", \"backbone.model.fc.weight\", \"backbone.model.fc.bias\", \"backbone.feature_extractor.layer1.1.weight\", \"backbone.feature_extractor.layer1.1.bias\", \"backbone.feature_extractor.layer1.1.running_mean\", \"backbone.feature_extractor.layer1.1.running_var\", \"backbone.feature_extractor.layer1.4.0.conv1.weight\", \"backbone.feature_extractor.layer1.4.0.bn1.weight\", \"backbone.feature_extractor.layer1.4.0.bn1.bias\", \"backbone.feature_extractor.layer1.4.0.bn1.running_mean\", \"backbone.feature_extractor.layer1.4.0.bn1.running_var\", \"backbone.feature_extractor.layer1.4.0.conv2.weight\", \"backbone.feature_extractor.layer1.4.0.bn2.weight\", \"backbone.feature_extractor.layer1.4.0.bn2.bias\", \"backbone.feature_extractor.layer1.4.0.bn2.running_mean\", \"backbone.feature_extractor.layer1.4.0.bn2.running_var\", \"backbone.feature_extractor.layer1.4.1.conv1.weight\", \"backbone.feature_extractor.layer1.4.1.bn1.weight\", \"backbone.feature_extractor.layer1.4.1.bn1.bias\", \"backbone.feature_extractor.layer1.4.1.bn1.running_mean\", \"backbone.feature_extractor.layer1.4.1.bn1.running_var\", \"backbone.feature_extractor.layer1.4.1.conv2.weight\", \"backbone.feature_extractor.layer1.4.1.bn2.weight\", \"backbone.feature_extractor.layer1.4.1.bn2.bias\", \"backbone.feature_extractor.layer1.4.1.bn2.running_mean\", \"backbone.feature_extractor.layer1.4.1.bn2.running_var\", \"backbone.feature_extractor.layer1.5.0.conv1.weight\", \"backbone.feature_extractor.layer1.5.0.bn1.weight\", \"backbone.feature_extractor.layer1.5.0.bn1.bias\", \"backbone.feature_extractor.layer1.5.0.bn1.running_mean\", \"backbone.feature_extractor.layer1.5.0.bn1.running_var\", \"backbone.feature_extractor.layer1.5.0.conv2.weight\", \"backbone.feature_extractor.layer1.5.0.bn2.weight\", \"backbone.feature_extractor.layer1.5.0.bn2.bias\", \"backbone.feature_extractor.layer1.5.0.bn2.running_mean\", \"backbone.feature_extractor.layer1.5.0.bn2.running_var\", \"backbone.feature_extractor.layer1.5.0.downsample.0.weight\", \"backbone.feature_extractor.layer1.5.0.downsample.1.weight\", \"backbone.feature_extractor.layer1.5.0.downsample.1.bias\", \"backbone.feature_extractor.layer1.5.0.downsample.1.running_mean\", \"backbone.feature_extractor.layer1.5.0.downsample.1.running_var\", \"backbone.feature_extractor.layer1.5.1.conv1.weight\", \"backbone.feature_extractor.layer1.5.1.bn1.weight\", \"backbone.feature_extractor.layer1.5.1.bn1.bias\", \"backbone.feature_extractor.layer1.5.1.bn1.running_mean\", \"backbone.feature_extractor.layer1.5.1.bn1.running_var\", \"backbone.feature_extractor.layer1.5.1.conv2.weight\", \"backbone.feature_extractor.layer1.5.1.bn2.weight\", \"backbone.feature_extractor.layer1.5.1.bn2.bias\", \"backbone.feature_extractor.layer1.5.1.bn2.running_mean\", \"backbone.feature_extractor.layer1.5.1.bn2.running_var\", \"backbone.feature_extractor.layer2.0.conv1.weight\", \"backbone.feature_extractor.layer2.0.bn1.weight\", \"backbone.feature_extractor.layer2.0.bn1.bias\", \"backbone.feature_extractor.layer2.0.bn1.running_mean\", \"backbone.feature_extractor.layer2.0.bn1.running_var\", \"backbone.feature_extractor.layer2.0.conv2.weight\", \"backbone.feature_extractor.layer2.0.bn2.weight\", \"backbone.feature_extractor.layer2.0.bn2.bias\", \"backbone.feature_extractor.layer2.0.bn2.running_mean\", \"backbone.feature_extractor.layer2.0.bn2.running_var\", \"backbone.feature_extractor.layer2.0.downsample.0.weight\", \"backbone.feature_extractor.layer2.0.downsample.1.weight\", \"backbone.feature_extractor.layer2.0.downsample.1.bias\", \"backbone.feature_extractor.layer2.0.downsample.1.running_mean\", \"backbone.feature_extractor.layer2.0.downsample.1.running_var\", \"backbone.feature_extractor.layer2.1.conv1.weight\", \"backbone.feature_extractor.layer2.1.bn1.weight\", \"backbone.feature_extractor.layer2.1.bn1.bias\", \"backbone.feature_extractor.layer2.1.bn1.running_mean\", \"backbone.feature_extractor.layer2.1.bn1.running_var\", \"backbone.feature_extractor.layer2.1.conv2.weight\", \"backbone.feature_extractor.layer2.1.bn2.weight\", \"backbone.feature_extractor.layer2.1.bn2.bias\", \"backbone.feature_extractor.layer2.1.bn2.running_mean\", \"backbone.feature_extractor.layer2.1.bn2.running_var\", \"backbone.feature_extractor.layer3.0.conv1.weight\", \"backbone.feature_extractor.layer3.0.bn1.weight\", \"backbone.feature_extractor.layer3.0.bn1.bias\", \"backbone.feature_extractor.layer3.0.bn1.running_mean\", \"backbone.feature_extractor.layer3.0.bn1.running_var\", \"backbone.feature_extractor.layer3.0.conv2.weight\", \"backbone.feature_extractor.layer3.0.bn2.weight\", \"backbone.feature_extractor.layer3.0.bn2.bias\", \"backbone.feature_extractor.layer3.0.bn2.running_mean\", \"backbone.feature_extractor.layer3.0.bn2.running_var\", \"backbone.feature_extractor.layer3.0.downsample.0.weight\", \"backbone.feature_extractor.layer3.0.downsample.1.weight\", \"backbone.feature_extractor.layer3.0.downsample.1.bias\", \"backbone.feature_extractor.layer3.0.downsample.1.running_mean\", \"backbone.feature_extractor.layer3.0.downsample.1.running_var\", \"backbone.feature_extractor.layer3.1.conv1.weight\", \"backbone.feature_extractor.layer3.1.bn1.weight\", \"backbone.feature_extractor.layer3.1.bn1.bias\", \"backbone.feature_extractor.layer3.1.bn1.running_mean\", \"backbone.feature_extractor.layer3.1.bn1.running_var\", \"backbone.feature_extractor.layer3.1.conv2.weight\", \"backbone.feature_extractor.layer3.1.bn2.weight\", \"backbone.feature_extractor.layer3.1.bn2.bias\", \"backbone.feature_extractor.layer3.1.bn2.running_mean\", \"backbone.feature_extractor.layer3.1.bn2.running_var\". \n\tUnexpected key(s) in state_dict: \"backbone.feature_extractor.layer1.6.weight\", \"backbone.feature_extractor.layer1.6.bias\", \"backbone.feature_extractor.layer1.8.weight\", \"backbone.feature_extractor.layer1.8.bias\", \"backbone.feature_extractor.layer1.9.weight\", \"backbone.feature_extractor.layer1.9.bias\", \"backbone.feature_extractor.layer1.9.running_mean\", \"backbone.feature_extractor.layer1.9.running_var\", \"backbone.feature_extractor.layer1.9.num_batches_tracked\", \"backbone.feature_extractor.layer1.0.bias\", \"backbone.feature_extractor.layer1.3.weight\", \"backbone.feature_extractor.layer1.3.bias\", \"backbone.feature_extractor.layer2.3.weight\", \"backbone.feature_extractor.layer2.3.bias\", \"backbone.feature_extractor.layer2.4.weight\", \"backbone.feature_extractor.layer2.4.bias\", \"backbone.feature_extractor.layer2.4.running_mean\", \"backbone.feature_extractor.layer2.4.running_var\", \"backbone.feature_extractor.layer2.4.num_batches_tracked\", \"backbone.feature_extractor.layer2.1.weight\", \"backbone.feature_extractor.layer2.1.bias\", \"backbone.feature_extractor.layer3.3.weight\", \"backbone.feature_extractor.layer3.3.bias\", \"backbone.feature_extractor.layer3.4.weight\", \"backbone.feature_extractor.layer3.4.bias\", \"backbone.feature_extractor.layer3.4.running_mean\", \"backbone.feature_extractor.layer3.4.running_var\", \"backbone.feature_extractor.layer3.4.num_batches_tracked\", \"backbone.feature_extractor.layer3.1.weight\", \"backbone.feature_extractor.layer3.1.bias\". \n\tsize mismatch for backbone.feature_extractor.layer1.0.weight: copying a param with shape torch.Size([32, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 3, 7, 7]).\n\tsize mismatch for backbone.feature_extractor.layer4.1.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3]).\n\tsize mismatch for backbone.feature_extractor.layer4.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for backbone.feature_extractor.layer4.3.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3]).\n\tsize mismatch for backbone.feature_extractor.layer4.3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for backbone.feature_extractor.layer4.4.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for backbone.feature_extractor.layer4.4.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for backbone.feature_extractor.layer4.4.running_mean: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for backbone.feature_extractor.layer4.4.running_var: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for backbone.feature_extractor.layer5.1.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3]).\n\tsize mismatch for backbone.feature_extractor.layer5.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for backbone.feature_extractor.layer5.3.weight: copying a param with shape torch.Size([128, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 512, 3, 3]).\n\tsize mismatch for backbone.feature_extractor.layer5.3.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for backbone.feature_extractor.layer5.4.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for backbone.feature_extractor.layer5.4.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for backbone.feature_extractor.layer5.4.running_mean: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for backbone.feature_extractor.layer5.4.running_var: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for backbone.feature_extractor.layer6.1.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3]).\n\tsize mismatch for backbone.feature_extractor.layer6.1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for backbone.feature_extractor.layer6.3.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 256, 3, 3]).\n\tsize mismatch for box_head.predictor.cls_headers.0.weight: copying a param with shape torch.Size([20, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([20, 128, 3, 3]).\n\tsize mismatch for box_head.predictor.cls_headers.1.weight: copying a param with shape torch.Size([30, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([30, 256, 3, 3]).\n\tsize mismatch for box_head.predictor.cls_headers.2.weight: copying a param with shape torch.Size([30, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([30, 512, 3, 3]).\n\tsize mismatch for box_head.predictor.cls_headers.3.weight: copying a param with shape torch.Size([30, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([30, 512, 3, 3]).\n\tsize mismatch for box_head.predictor.cls_headers.4.weight: copying a param with shape torch.Size([20, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([20, 256, 3, 3]).\n\tsize mismatch for box_head.predictor.reg_headers.0.weight: copying a param with shape torch.Size([16, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 128, 3, 3]).\n\tsize mismatch for box_head.predictor.reg_headers.1.weight: copying a param with shape torch.Size([24, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([24, 256, 3, 3]).\n\tsize mismatch for box_head.predictor.reg_headers.2.weight: copying a param with shape torch.Size([24, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([24, 512, 3, 3]).\n\tsize mismatch for box_head.predictor.reg_headers.3.weight: copying a param with shape torch.Size([24, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([24, 512, 3, 3]).\n\tsize mismatch for box_head.predictor.reg_headers.4.weight: copying a param with shape torch.Size([16, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 256, 3, 3])."
     ]
    }
   ],
   "source": [
    "#config_file = \"configs/vgg_ssd300_voc0712_tdt4265_server.yaml\"\n",
    "config_file = \"configs/train_rdd2020_server.yaml\"\n",
    "ckpt = None # The path to the checkpoint for test, default is the latest checkpoint\n",
    "score_threshold = 0.1\n",
    "images_dir = \"demo/rdd2020\" # chage to \"demo/mnist\" for MNIST\n",
    "dataset_type = \"rdd2020\" # change to \"mnist\" for MNIST.\n",
    "\n",
    "cfg.merge_from_file(config_file)\n",
    "cfg.freeze()\n",
    "\n",
    "print(\"Loaded configuration file {}\".format(config_file))\n",
    "with open(config_file, \"r\") as cf:\n",
    "    config_str = \"\\n\" + cf.read()\n",
    "print(\"Running with config:\\n{}\".format(cfg))\n",
    "\n",
    "drawn_images = run_demo(\n",
    "    cfg=cfg,ckpt=ckpt,\n",
    "    score_threshold=score_threshold,\n",
    "    images_dir=pathlib.Path(images_dir),\n",
    "    output_dir=pathlib.Path(images_dir, \"result\"),\n",
    "    dataset_type=dataset_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example to plot the first drawn image\n",
    "print(\"Number of images:\", len(drawn_images))\n",
    "plt.imshow(drawn_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(drawn_images[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}